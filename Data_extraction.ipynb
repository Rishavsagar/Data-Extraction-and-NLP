{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3NuPbveJaL/BKj1prLZij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishavsagar/Data-Extraction-and-NLP/blob/main/Data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-SgDNABgZW-",
        "outputId": "057db745-8481-47af-c50f-b0cffc49edae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            URL_ID                                              Title  \\\n",
            "0  Netclan20241017  AI and ML-Based YouTube Analytics and Content ...   \n",
            "1  Netclan20241018  Enhancing Front-End Features and Functionality...   \n",
            "2  Netclan20241019  ROAS Dashboard for Campaign-Wise Google Ads Bu...   \n",
            "3  Netclan20241020  Efficient Processing and Analysis of Financial...   \n",
            "4  Netclan20241021      Development of EA Robot for Automated Trading   \n",
            "\n",
            "                                                Text  \n",
            "0  Transforming Real Estate Investments with an A...  \n",
            "1  Transforming Real Estate Investments with an A...  \n",
            "2  Transforming Real Estate Investments with an A...  \n",
            "3  Transforming Real Estate Investments with an A...  \n",
            "4  Transforming Real Estate Investments with an A...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load Input.xlsx\n",
        "input_data = pd.read_excel(\"Input.xlsx\")\n",
        "\n",
        "# Function to extract title and article text\n",
        "def extract_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        title = soup.find(\"h1\").get_text(strip=True)\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        article_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Extract data for each URL\n",
        "extracted_data = []\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row[\"URL_ID\"]\n",
        "    url = row[\"URL\"]\n",
        "    title, text = extract_text(url)\n",
        "    extracted_data.append({\"URL_ID\": url_id, \"Title\": title, \"Text\": text})\n",
        "\n",
        "# Save extracted data to a DataFrame\n",
        "extracted_df = pd.DataFrame(extracted_data)\n",
        "print(extracted_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the extracted text into .txt\n",
        "for index, row in extracted_df.iterrows():\n",
        "    if row[\"Text\"]:\n",
        "        with open(f\"{row['URL_ID']}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"{row['Title']}\\n{row['Text']}\")\n"
      ],
      "metadata": {
        "id": "nHwcTEeHjVrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install textstat nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"stopwords\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK_Qbn6olBy4",
        "outputId": "42c79475-d5ca-4b84-ed8c-8a6b74e82ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the MasterDictionary folder\n",
        "import os\n",
        "os.makedirs(\"MasterDictionary\", exist_ok=True)\n",
        "\n",
        "# Download the positive and negative word lists\n",
        "!wget -O MasterDictionary/positive-words.txt https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/positive-words.txt\n",
        "!wget -O MasterDictionary/negative-words.txt https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/negative-words.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxTIHx1MrLSu",
        "outputId": "bccfd43a-0bc9-4eaa-859c-c8a250e45e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-10 12:28:57--  https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/positive-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20630 (20K) [text/plain]\n",
            "Saving to: ‘MasterDictionary/positive-words.txt’\n",
            "\n",
            "MasterDictionary/po 100%[===================>]  20.15K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-04-10 12:28:57 (13.0 MB/s) - ‘MasterDictionary/positive-words.txt’ saved [20630/20630]\n",
            "\n",
            "--2025-04-10 12:28:58--  https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/negative-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46299 (45K) [application/octet-stream]\n",
            "Saving to: ‘MasterDictionary/negative-words.txt’\n",
            "\n",
            "MasterDictionary/ne 100%[===================>]  45.21K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-04-10 12:28:58 (5.18 MB/s) - ‘MasterDictionary/negative-words.txt’ saved [46299/46299]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import textstat\n",
        "import re\n",
        "\n",
        "# Load positive and negative word lists\n",
        "def load_words(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "        return set([line.strip().lower() for line in f if line.strip() and not line.startswith(\";\")])\n",
        "\n",
        "positive_words = load_words(\"MasterDictionary/positive-words.txt\")\n",
        "negative_words = load_words(\"MasterDictionary/negative-words.txt\")\n",
        "\n",
        "def analyze_text(text):\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    cleaned_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Sentiment\n",
        "    positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
        "    negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
        "\n",
        "    # Readability\n",
        "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
        "    percentage_complex_words = sum(1 for word in cleaned_words if textstat.syllable_count(word) > 2) / len(cleaned_words) * 100 if cleaned_words else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Other\n",
        "    complex_word_count = sum(1 for word in cleaned_words if textstat.syllable_count(word) > 2)\n",
        "    syllables_per_word = sum(textstat.syllable_count(word) for word in cleaned_words) / len(cleaned_words) if cleaned_words else 0\n",
        "    personal_pronouns = len(re.findall(r\"\\b(I|we|my|ours|us)\\b\", text, re.I))\n",
        "    avg_word_length = sum(len(word) for word in cleaned_words) / len(cleaned_words) if cleaned_words else 0\n",
        "\n",
        "    return [\n",
        "        positive_score,\n",
        "        negative_score,\n",
        "        polarity_score,\n",
        "        subjectivity_score,\n",
        "        avg_sentence_length,\n",
        "        percentage_complex_words,\n",
        "        fog_index,\n",
        "        avg_sentence_length,\n",
        "        complex_word_count,\n",
        "        len(word_tokenize(text)),\n",
        "        syllables_per_word,\n",
        "        personal_pronouns,\n",
        "        avg_word_length\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "Rw54nQWCriWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load input file\n",
        "df_input = pd.read_excel(\"Input.xlsx\")\n",
        "\n",
        "# Define output structure\n",
        "output_columns = [\n",
        "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "    'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
        "]\n",
        "\n",
        "output_rows = []\n",
        "\n",
        "for _, row in df_input.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    try:\n",
        "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "            metrics = analyze_text(text)\n",
        "            output_rows.append([url_id, url] + metrics)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for URL_ID: {url_id}\")\n",
        "        output_rows.append([url_id, url] + [0]*13)\n",
        "\n",
        "df_output = pd.DataFrame(output_rows, columns=output_columns)\n",
        "\n",
        "# Save as Excel\n",
        "df_output.to_excel(\"Output Data Structure.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "CJWl-ycurrJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"Output Data Structure.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GXs1IhgTrxVw",
        "outputId": "832741f6-30a2-4936-ad0e-cb51dc7996ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f1da480-66d8-49c8-9a02-4cbb5eeb0322\", \"Output Data Structure.xlsx\", 27940)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}